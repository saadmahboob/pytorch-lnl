{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "",
  "signature": "sha256:add55e185ab109a37a770aa9f63d8fe59a309a852a4a801fd4477d088bdf2385"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# imports\n",
      "import torch\n",
      "from torch import nn\n",
      "from torch.optim import SGD\n",
      "from torch.utils.data import DataLoader\n",
      "from torch.autograd import Variable\n",
      "from torchvision.datasets import MNIST\n",
      "from torch.utils.data import DataLoader\n",
      "from torchvision.transforms import ToTensor, Normalize, Compose\n",
      "from IPython.display import Image\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
      "init_notebook_mode(connected=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x7fd845ca8950>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# summary of neural networks\n",
      "Image(url='https://i.imgur.com/ktUOnca.jpg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"https://i.imgur.com/ktUOnca.jpg\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<IPython.core.display.Image at 0x7fd8f5e0f4d0>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# What is Deep Learning?\n",
      "\n",
      "A machine learning strategy for learning representations (embeddings) of data.\n",
      "\n",
      "* **Input:** An input vector for a piece of data you wish to represent.\n",
      "* **Process:** *magic*\n",
      "* **Output:** An embedding for your data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# *magic* ?!?\n",
      "\n",
      "Yes.\n",
      "\n",
      "The process is a neural network. It applies a sequence of functions to the input:\n",
      "\n",
      "* **Input:** $x$\n",
      "* **Output:** $f_n(f_{n-1}(\\dots f_2(f_1(x))\\dots))$\n",
      "* Traditionally, each $f_i$ is a matrix multiplication followed by a nonlinearity (called an *activation function*).\n",
      "* Each fi can basically be anything as long as it is **differentiable**.\n",
      "* We call these $f_i$\u2019s *layers*.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example: Basic Linear Layer with ReLU activation function.\n",
      "\n",
      "Let's let \n",
      "* $f_1(x) = \\begin{bmatrix} 1 & 2 \\\\ 4 & 3 \\end{bmatrix}x + \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$, \n",
      "* $f_2(x) = max(0, x)$,\n",
      "* $x = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$\n",
      "\n",
      "So $f_2(f_1(x)) = f_2(\\begin{bmatrix} -1 \\\\ 2\\end{bmatrix})=\\begin{bmatrix}0 \\\\ 2\\end{bmatrix}$\n",
      "\n",
      "* Here, $f_1$ is called a linear layer and $f_2$ is called the ReLU activation function.\n",
      "* Linear layers needn't have the same number of inputs and outputs.\n",
      "* Typically the parameters inside of the matrices of $f_1$ are *learned* in training over a dataset.\n",
      "* This learning is done with backpropogation, and is the reason that we need our layers to be differentiable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## idea behind backprop\n",
      "* You have an input $x$ with a target output $t$.\n",
      "* Pass the input $x$ through the series of layers to get an output $y=f_n(\\dots f_1(x)\\dots)$\n",
      "* Calculate some error $e$ on $y$ with respect to the target output $t$ (e.g. from classification or regression).\n",
      "* That error is called the *loss* and you want to learn parameters in your $f_i$'s that minimize it.\n",
      "### **WARNING!!! MATH AHEAD**\n",
      "* For each parameter $p$ in your $f_i$'s, you can calculate the derivative $\\frac{de}{dp}$ via repeated use of the chain rule and use that derivative (called a gradient) to update the parameter $p$ to reduce your loss.\n",
      "* this process is called *backpropogation*, since it moves backwards through the network.\n",
      "* Since the derivative of a parameter is calculated with respect to the derivatives of all downstream parameters, any parameter that depends on a parameter which is learned needs a gradient.\n",
      "\n",
      "### Practical notes:\n",
      "* A loss is typically defined as the sum of the individual losses over the entire dataset.\n",
      "* It is too expensive to compute the loss for the entire dataset before updating the parameters.\n",
      "* Updating parameters for every single example tends to be too chaotic to get the model to converge.\n",
      "* We instead process the data in random batches as a hybrid of the two approaches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url=\"http://www.frank-dieterle.de/phd/images/image016.gif\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://www.frank-dieterle.de/phd/images/image016.gif\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "<IPython.core.display.Image at 0x7fd8f5e0f610>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at how this works in PyTorch"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# how tensors work\n",
      "x = torch.zeros(2,3)\n",
      "x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "\n",
        " 0  0  0\n",
        " 0  0  0\n",
        "[torch.FloatTensor of size 2x3]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# how variables work\n",
      "x = Variable(x)\n",
      "print \"x:\", x\n",
      "print \"requires grad:\", x.requires_grad\n",
      "print \"data:\", x.data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x: Variable containing:\n",
        " 0  0  0\n",
        " 0  0  0\n",
        "[torch.FloatTensor of size 2x3]\n",
        "\n",
        "requires grad: False\n",
        "data: \n",
        " 0  0  0\n",
        " 0  0  0\n",
        "[torch.FloatTensor of size 2x3]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = x + 1\n",
      "z = Variable(torch.ones(2,3), requires_grad=True)\n",
      "w = y+z\n",
      "\n",
      "print y\n",
      "print \"does y require grad?\", y.requires_grad\n",
      "print \"does w require grad?\", w.requires_grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Variable containing:\n",
        " 1  1  1\n",
        " 1  1  1\n",
        "[torch.FloatTensor of size 2x3]\n",
        "\n",
        "does y require grad? False\n",
        "does w require grad? True\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what about neural network layers?\n",
      "lin = nn.Linear(3, 4)  # 3 inputs and 4 outputs. Should contain a 4x3 matrix and a 4x1 matrix of parameters.\n",
      "list(lin.parameters())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[Parameter containing:\n",
        " -0.1154  0.0520  0.4254\n",
        "  0.0138  0.2027 -0.0502\n",
        "  0.4299 -0.1628 -0.4917\n",
        "  0.0246 -0.2720  0.0590\n",
        " [torch.FloatTensor of size 4x3], Parameter containing:\n",
        " -0.0574\n",
        " -0.1704\n",
        " -0.0304\n",
        " -0.4828\n",
        " [torch.FloatTensor of size 4]]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"lin output\", lin(y)  # processing with a batch size of 2.\n",
      "print lin(y).requires_grad\n",
      "print lin(w).requires_grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "lin output Variable containing:\n",
        " 0.3046 -0.0041 -0.2550 -0.6712\n",
        " 0.3046 -0.0041 -0.2550 -0.6712\n",
        "[torch.FloatTensor of size 2x4]\n",
        "\n",
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "relu = nn.ReLU()\n",
      "print relu(lin(y))\n",
      "print relu(lin(y)).requires_grad\n",
      "print relu(w).requires_grad\n",
      "print relu(y).requires_grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Variable containing:\n",
        " 0.3046  0.0000  0.0000  0.0000\n",
        " 0.3046  0.0000  0.0000  0.0000\n",
        "[torch.FloatTensor of size 2x4]\n",
        "\n",
        "True\n",
        "True\n",
        "False\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Kinds of data and their layers:\n",
      "## Convolutional layers (good for image data).\n",
      "* An $m\\times n$ image with $3$ color channels is $m\\times n \\times 3$ dimensional. \n",
      "* For $m=n=200$, $m\\times n \\times 3 = 120000$, which is a lot of inputs for a linear layer to take in right away. Suppose you wanted to have an output dimension of 64. That would require $120000\\times 64 + 64 = 7680064$ parameters.\n",
      "* **Idea:** why not use the fact that patches of regions across the image have similar patterns? If we use this fact, then a lot of weights over the entire image can actually be the same.\n",
      "* Let's focus on learning to get good with $3\\times 3$ patches for now...\n",
      "* A convolutional network over $3\\times 3$ patches with the same number of inputs and outputs as above will have a set of $3 \\times 3 \\times 3 \\times 64 + 64= 1792$ parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url=\"https://i.stack.imgur.com/GvsBA.jpg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"https://i.stack.imgur.com/GvsBA.jpg\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<IPython.core.display.Image at 0x7fd844cbc490>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Recurrent layers (good for sequential data).\n",
      "* Suppose you have a training set of sentences for sentiment classification.\n",
      "* Each sentence has either positive, negative, or neutral sentiment.\n",
      "* How do you feed that through a model?\n",
      "* Concatenate the word2vec word embeddings for the words together and use that as input?\n",
      "* **Problem:** sentences are variable length. \n",
      "* You could just grab the first $N$ words of the sentence, but that probably won't do too well at detecting sentiment. Also, you'd have an input size of $N\\times m$ where $m$ is the size of your word embeddings.\n",
      "### Alternative approach: use a recurrent layer.\n",
      "* Holds an internal *state* vector for the built up representation of the sequence up until that point.\n",
      "* Could process word embeddings one at a time and learn to output the sentiment at the end."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<IPython.core.display.Image at 0x7fd8f5e0f410>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setting up datasets.\n",
      "training = DataLoader(\n",
      "    MNIST(\n",
      "        \"/home/temerick/train\", \n",
      "        download=False, \n",
      "        transform=Compose([ToTensor(),Normalize((0.1307,), (0.3081,))])\n",
      "    ), \n",
      "    batch_size=64, \n",
      "    shuffle=True, \n",
      "    num_workers=40\n",
      ")\n",
      "testing = DataLoader(\n",
      "    MNIST(\n",
      "        \"/home/temerick/test\", \n",
      "        train=False,\n",
      "        download=False, \n",
      "        transform=Compose([ToTensor(),Normalize((0.1307,), (0.3081,))])\n",
      "    ), \n",
      "    batch_size=64, \n",
      "    shuffle=True, \n",
      "    num_workers=40\n",
      ")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(training.dataset), len(testing.dataset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "(60000, 10000)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print MNIST('/home/temerick/train')[0]\n",
      "MNIST('/home/temerick/train')[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(<PIL.Image.Image image mode=L size=28x28 at 0x7FD8F5DFE750>, 5)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy\n/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/H\ntn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+\n/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/f\nv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y3\n5wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
       "prompt_number": 14,
       "text": [
        "<PIL.Image.Image image mode=L size=28x28 at 0x7FD844A6C090>"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# basic linear with relu's\n",
      "class LinReLU(nn.Module):\n",
      "    def __init__(self, indim, outdim):\n",
      "        super(LinReLU, self).__init__()\n",
      "        self.lin = nn.Linear(indim, outdim)\n",
      "        self.relu = nn.ReLU(True)\n",
      "    \n",
      "    def forward(self, input):\n",
      "        input = self.lin(input)\n",
      "        return self.relu(input)\n",
      "\n",
      "\n",
      "class MnistLinear(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(MnistLinear, self).__init__()\n",
      "        self.lin1 = LinReLU(28*28, 16)\n",
      "        self.lin2 = nn.Linear(16, 10)\n",
      "    \n",
      "    def forward(self, input):\n",
      "        input = self.lin1(input.view(-1, 28*28))\n",
      "        input = self.lin2(input)\n",
      "        return input"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ConvReLU(nn.Module):\n",
      "    def __init__(self, in_channels, out_channels, kernel=3):\n",
      "        super(ConvReLU, self).__init__()\n",
      "        self.conv = nn.Conv2d(in_channels, out_channels, kernel, padding=kernel/2)\n",
      "        self.relu = nn.ReLU(True)\n",
      "        self.bn = nn.BatchNorm2d(out_channels)\n",
      "    \n",
      "    def forward(self, input):\n",
      "        input = self.conv(input)\n",
      "        input = self.relu(input)\n",
      "        return self.bn(input)\n",
      "\n",
      "\n",
      "class MnistCNN(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(MnistCNN, self).__init__()\n",
      "        self.conv1 = ConvReLU(1, 16)\n",
      "        self.mp = nn.MaxPool2d(2)\n",
      "        self.conv2 = ConvReLU(16, 32)\n",
      "        self.avgpool = nn.AvgPool2d(14)\n",
      "        self.lin = nn.Linear(32, 10)\n",
      "    \n",
      "    def forward(self, input):\n",
      "        input = self.conv1(input)\n",
      "        input = self.mp(input)\n",
      "        input = self.conv2(input)\n",
      "        input = self.avgpool(input).squeeze()\n",
      "        return self.lin(input)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MnistRNN(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(MnistRNN, self).__init__()\n",
      "        self.lstm = nn.LSTM(\n",
      "            input_size=28, \n",
      "            hidden_size=16, \n",
      "            num_layers=2, \n",
      "            batch_first=True, \n",
      "            dropout=0.5\n",
      "        )\n",
      "        self.linear = nn.Linear(16, 10)\n",
      "    \n",
      "    def forward(self, input):\n",
      "        input, hidden = self.lstm(input.squeeze())\n",
      "        return self.linear(input[:, -1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ComboModel(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(ComboModel, self).__init__()\n",
      "        self.lin = MnistLinear()\n",
      "        self.cnn = MnistCNN()\n",
      "        self.rnn = MnistRNN()\n",
      "    \n",
      "    def forward(self, input):\n",
      "        l1 = self.lin(input)\n",
      "        l2 = self.cnn(input)\n",
      "        l3 = self.rnn(input)\n",
      "        return l1, l2, l3 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = ComboModel()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def num_parameters(m):\n",
      "    return sum([y.nelement() for y in m.parameters()])\n",
      "\n",
      "print \"num params for linear:\\t\", num_parameters(model.lin)\n",
      "print \"num params for cnn:\\t\", num_parameters(model.cnn)\n",
      "print \"num params for rnn:\\t\", num_parameters(model.rnn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "num params for linear:\t12730\n",
        "num params for cnn:\t5226\n",
        "num params for rnn:\t5290\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = model.cuda(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loss = nn.CrossEntropyLoss().cuda(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train(epoch):\n",
      "    model.train()\n",
      "    for batch_idx, (data, target) in enumerate(training):\n",
      "        data, target = data.cuda(1), target.cuda(1)\n",
      "        data, target = Variable(data), Variable(target)\n",
      "        optimizer.zero_grad()\n",
      "        lin_out, cnn_out, rnn_out = model(data)\n",
      "        lin_loss = loss(lin_out, target)\n",
      "        cnn_loss = loss(cnn_out, target)\n",
      "        rnn_loss = loss(rnn_out, target)\n",
      "        total_loss = lin_loss + cnn_loss + rnn_loss\n",
      "        total_loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch_idx % 100 == 0:\n",
      "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLin Loss: {:.6f}\\tCNN Loss: {:.6f}\\tRNN Loss: {:.6f}'.format(\n",
      "                epoch+1, batch_idx, len(training),\n",
      "                100. * batch_idx / len(training), lin_loss.data[0], cnn_loss.data[0], rnn_loss.data[0]))\n",
      "\n",
      "num_epochs = 10\n",
      "losses = [{}] * (num_epochs+1)\n",
      "correct = [{}] * (num_epochs+1)\n",
      "\n",
      "def test(epoch):\n",
      "    model.eval()\n",
      "    losses[epoch] = dict(\n",
      "        Linear=0,\n",
      "        CNN=0,\n",
      "        RNN=0\n",
      "    )\n",
      "    correct[epoch] = dict(\n",
      "        Linear=0,\n",
      "        CNN=0,\n",
      "        RNN=0\n",
      "    )\n",
      "    for data, target in testing:\n",
      "        data, target = data.cuda(1), target.cuda(1)\n",
      "        data, target = Variable(data, volatile=True), Variable(target)\n",
      "        lin_out, cnn_out, rnn_out = model(data)\n",
      "        \n",
      "        for name, output in [(\"Linear\", lin_out), (\"CNN\", cnn_out), (\"RNN\", rnn_out)]:\n",
      "            losses[epoch][name] += loss(output, target).data[0]\n",
      "            pred = output.data.max(1)[1] \n",
      "            correct[epoch][name] += pred.eq(target.data).cpu().sum()\n",
      "\n",
      "    for name in ['Linear', 'CNN', 'RNN']:\n",
      "        losses[epoch][name] /= len(testing) # loss function already averages over batch size\n",
      "        print('{} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
      "            name, losses[epoch][name], correct[epoch][name], len(testing.dataset),\n",
      "            100. * correct[epoch][name] / len(testing.dataset)))\n",
      "        \n",
      "for epoch in range(num_epochs+1):\n",
      "    test(epoch)\n",
      "    train(epoch)\n",
      "test(num_epochs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 2.3217, Accuracy: 1048/10000 (10%)\n",
        "\n",
        "CNN Test set: Average loss: 2.3054, Accuracy: 1028/10000 (10%)\n",
        "\n",
        "RNN Test set: Average loss: 2.3086, Accuracy: 1157/10000 (12%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 1 [0/938 (0%)]\tLin Loss: 2.217525\tCNN Loss: 2.314173\tRNN Loss: 2.316472\n",
        "Train Epoch: 1 [100/938 (11%)]\tLin Loss: 0.530270\tCNN Loss: 1.503205\tRNN Loss: 2.212691"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 1 [200/938 (21%)]\tLin Loss: 0.358970\tCNN Loss: 0.786356\tRNN Loss: 1.477564"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 1 [300/938 (32%)]\tLin Loss: 0.434890\tCNN Loss: 0.570544\tRNN Loss: 1.427647"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 1 [400/938 (43%)]\tLin Loss: 0.148071\tCNN Loss: 0.210228\tRNN Loss: 0.771015"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 1 [500/938 (53%)]\tLin Loss: 0.226942\tCNN Loss: 0.142651\tRNN Loss: 0.627899"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 1 [600/938 (64%)]\tLin Loss: 0.261520\tCNN Loss: 0.198108\tRNN Loss: 0.528717"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 1 [700/938 (75%)]\tLin Loss: 0.155262\tCNN Loss: 0.233828\tRNN Loss: 0.556121"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 1 [800/938 (85%)]\tLin Loss: 0.177497\tCNN Loss: 0.133062\tRNN Loss: 0.366879"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 1 [900/938 (96%)]\tLin Loss: 0.267660\tCNN Loss: 0.132548\tRNN Loss: 0.457696"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2685, Accuracy: 9232/10000 (92%)\n",
        "\n",
        "CNN Test set: Average loss: 0.1476, Accuracy: 9598/10000 (96%)\n",
        "\n",
        "RNN Test set: Average loss: 0.3531, Accuracy: 8907/10000 (89%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 2 [0/938 (0%)]\tLin Loss: 0.143680\tCNN Loss: 0.072925\tRNN Loss: 0.455444\n",
        "Train Epoch: 2 [100/938 (11%)]\tLin Loss: 0.239492\tCNN Loss: 0.089046\tRNN Loss: 0.353362"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 2 [200/938 (21%)]\tLin Loss: 0.180465\tCNN Loss: 0.116604\tRNN Loss: 0.426111"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 2 [300/938 (32%)]\tLin Loss: 0.417887\tCNN Loss: 0.178395\tRNN Loss: 0.365226"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 2 [400/938 (43%)]\tLin Loss: 0.122988\tCNN Loss: 0.068502\tRNN Loss: 0.336707"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 2 [500/938 (53%)]\tLin Loss: 0.177882\tCNN Loss: 0.101063\tRNN Loss: 0.349846"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 2 [600/938 (64%)]\tLin Loss: 0.258518\tCNN Loss: 0.169159\tRNN Loss: 0.308784"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 2 [700/938 (75%)]\tLin Loss: 0.377506\tCNN Loss: 0.075117\tRNN Loss: 0.189114"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 2 [800/938 (85%)]\tLin Loss: 0.233631\tCNN Loss: 0.178688\tRNN Loss: 0.227602"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 2 [900/938 (96%)]\tLin Loss: 0.258676\tCNN Loss: 0.229414\tRNN Loss: 0.236877"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2516, Accuracy: 9303/10000 (93%)\n",
        "\n",
        "CNN Test set: Average loss: 0.1188, Accuracy: 9669/10000 (97%)\n",
        "\n",
        "RNN Test set: Average loss: 0.1867, Accuracy: 9449/10000 (94%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 3 [0/938 (0%)]\tLin Loss: 0.139950\tCNN Loss: 0.068313\tRNN Loss: 0.153769\n",
        "Train Epoch: 3 [100/938 (11%)]\tLin Loss: 0.237019\tCNN Loss: 0.173735\tRNN Loss: 0.345021"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 3 [200/938 (21%)]\tLin Loss: 0.203209\tCNN Loss: 0.141800\tRNN Loss: 0.214570"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 3 [300/938 (32%)]\tLin Loss: 0.086939\tCNN Loss: 0.017854\tRNN Loss: 0.173068"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 3 [400/938 (43%)]\tLin Loss: 0.137633\tCNN Loss: 0.037694\tRNN Loss: 0.082989"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 3 [500/938 (53%)]\tLin Loss: 0.178937\tCNN Loss: 0.066941\tRNN Loss: 0.148899"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 3 [600/938 (64%)]\tLin Loss: 0.221300\tCNN Loss: 0.106169\tRNN Loss: 0.152935"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 3 [700/938 (75%)]\tLin Loss: 0.474697\tCNN Loss: 0.137073\tRNN Loss: 0.119869"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 3 [800/938 (85%)]\tLin Loss: 0.182273\tCNN Loss: 0.072219\tRNN Loss: 0.146357"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 3 [900/938 (96%)]\tLin Loss: 0.066319\tCNN Loss: 0.036903\tRNN Loss: 0.147223"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2483, Accuracy: 9324/10000 (93%)\n",
        "\n",
        "CNN Test set: Average loss: 0.1212, Accuracy: 9635/10000 (96%)\n",
        "\n",
        "RNN Test set: Average loss: 0.1649, Accuracy: 9527/10000 (95%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 4 [0/938 (0%)]\tLin Loss: 0.335852\tCNN Loss: 0.191011\tRNN Loss: 0.310766\n",
        "Train Epoch: 4 [100/938 (11%)]\tLin Loss: 0.158072\tCNN Loss: 0.131326\tRNN Loss: 0.136589"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 4 [200/938 (21%)]\tLin Loss: 0.141524\tCNN Loss: 0.154583\tRNN Loss: 0.238179"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 4 [300/938 (32%)]\tLin Loss: 0.134243\tCNN Loss: 0.089055\tRNN Loss: 0.212843"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 4 [400/938 (43%)]\tLin Loss: 0.288866\tCNN Loss: 0.089417\tRNN Loss: 0.204949"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 4 [500/938 (53%)]\tLin Loss: 0.166559\tCNN Loss: 0.102768\tRNN Loss: 0.240154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 4 [600/938 (64%)]\tLin Loss: 0.196593\tCNN Loss: 0.057916\tRNN Loss: 0.102818"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 4 [700/938 (75%)]\tLin Loss: 0.193368\tCNN Loss: 0.025413\tRNN Loss: 0.257126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 4 [800/938 (85%)]\tLin Loss: 0.285011\tCNN Loss: 0.069592\tRNN Loss: 0.097611"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 4 [900/938 (96%)]\tLin Loss: 0.280652\tCNN Loss: 0.087233\tRNN Loss: 0.196838"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2262, Accuracy: 9393/10000 (94%)\n",
        "\n",
        "CNN Test set: Average loss: 0.0966, Accuracy: 9706/10000 (97%)\n",
        "\n",
        "RNN Test set: Average loss: 0.1357, Accuracy: 9607/10000 (96%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 5 [0/938 (0%)]\tLin Loss: 0.177160\tCNN Loss: 0.059294\tRNN Loss: 0.138237\n",
        "Train Epoch: 5 [100/938 (11%)]\tLin Loss: 0.204689\tCNN Loss: 0.068655\tRNN Loss: 0.099576"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 5 [200/938 (21%)]\tLin Loss: 0.144583\tCNN Loss: 0.022162\tRNN Loss: 0.183531"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 5 [300/938 (32%)]\tLin Loss: 0.202784\tCNN Loss: 0.025534\tRNN Loss: 0.151822"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 5 [400/938 (43%)]\tLin Loss: 0.206329\tCNN Loss: 0.120087\tRNN Loss: 0.065789"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 5 [500/938 (53%)]\tLin Loss: 0.245835\tCNN Loss: 0.076945\tRNN Loss: 0.349397"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 5 [600/938 (64%)]\tLin Loss: 0.177255\tCNN Loss: 0.132089\tRNN Loss: 0.175043"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 5 [700/938 (75%)]\tLin Loss: 0.336783\tCNN Loss: 0.073447\tRNN Loss: 0.116268"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 5 [800/938 (85%)]\tLin Loss: 0.147422\tCNN Loss: 0.048506\tRNN Loss: 0.171943"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 5 [900/938 (96%)]\tLin Loss: 0.436193\tCNN Loss: 0.064907\tRNN Loss: 0.254182"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2337, Accuracy: 9354/10000 (94%)\n",
        "\n",
        "CNN Test set: Average loss: 0.0802, Accuracy: 9760/10000 (98%)\n",
        "\n",
        "RNN Test set: Average loss: 0.1113, Accuracy: 9657/10000 (97%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 6 [0/938 (0%)]\tLin Loss: 0.244037\tCNN Loss: 0.143479\tRNN Loss: 0.206429\n",
        "Train Epoch: 6 [100/938 (11%)]\tLin Loss: 0.143145\tCNN Loss: 0.144977\tRNN Loss: 0.190553"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 6 [200/938 (21%)]\tLin Loss: 0.282543\tCNN Loss: 0.049701\tRNN Loss: 0.086151"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 6 [300/938 (32%)]\tLin Loss: 0.181863\tCNN Loss: 0.159730\tRNN Loss: 0.185690"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 6 [400/938 (43%)]\tLin Loss: 0.312557\tCNN Loss: 0.158001\tRNN Loss: 0.242771"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 6 [500/938 (53%)]\tLin Loss: 0.079055\tCNN Loss: 0.023140\tRNN Loss: 0.171782"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 6 [600/938 (64%)]\tLin Loss: 0.212696\tCNN Loss: 0.032070\tRNN Loss: 0.186237"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 6 [700/938 (75%)]\tLin Loss: 0.202546\tCNN Loss: 0.022731\tRNN Loss: 0.112067"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 6 [800/938 (85%)]\tLin Loss: 0.113626\tCNN Loss: 0.034063\tRNN Loss: 0.102212"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 6 [900/938 (96%)]\tLin Loss: 0.223073\tCNN Loss: 0.130620\tRNN Loss: 0.106751"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2501, Accuracy: 9360/10000 (94%)\n",
        "\n",
        "CNN Test set: Average loss: 0.0786, Accuracy: 9770/10000 (98%)\n",
        "\n",
        "RNN Test set: Average loss: 0.0998, Accuracy: 9712/10000 (97%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 7 [0/938 (0%)]\tLin Loss: 0.100845\tCNN Loss: 0.171558\tRNN Loss: 0.109001\n",
        "Train Epoch: 7 [100/938 (11%)]\tLin Loss: 0.104618\tCNN Loss: 0.059715\tRNN Loss: 0.134351"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 7 [200/938 (21%)]\tLin Loss: 0.129489\tCNN Loss: 0.136231\tRNN Loss: 0.128811"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 7 [300/938 (32%)]\tLin Loss: 0.243465\tCNN Loss: 0.085706\tRNN Loss: 0.265648"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 7 [400/938 (43%)]\tLin Loss: 0.158982\tCNN Loss: 0.057978\tRNN Loss: 0.154029"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 7 [500/938 (53%)]\tLin Loss: 0.201837\tCNN Loss: 0.048771\tRNN Loss: 0.084046"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 7 [600/938 (64%)]\tLin Loss: 0.225559\tCNN Loss: 0.062097\tRNN Loss: 0.120043"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 7 [700/938 (75%)]\tLin Loss: 0.308488\tCNN Loss: 0.047850\tRNN Loss: 0.190833"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 7 [800/938 (85%)]\tLin Loss: 0.231864\tCNN Loss: 0.064536\tRNN Loss: 0.145235"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 7 [900/938 (96%)]\tLin Loss: 0.136118\tCNN Loss: 0.113468\tRNN Loss: 0.115765"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2369, Accuracy: 9346/10000 (93%)\n",
        "\n",
        "CNN Test set: Average loss: 0.0766, Accuracy: 9771/10000 (98%)\n",
        "\n",
        "RNN Test set: Average loss: 0.1020, Accuracy: 9699/10000 (97%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 8 [0/938 (0%)]\tLin Loss: 0.189384\tCNN Loss: 0.056070\tRNN Loss: 0.043224\n",
        "Train Epoch: 8 [100/938 (11%)]\tLin Loss: 0.188840\tCNN Loss: 0.209481\tRNN Loss: 0.107297"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 8 [200/938 (21%)]\tLin Loss: 0.217793\tCNN Loss: 0.104685\tRNN Loss: 0.045693"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 8 [300/938 (32%)]\tLin Loss: 0.356494\tCNN Loss: 0.043383\tRNN Loss: 0.337282"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 8 [400/938 (43%)]\tLin Loss: 0.218571\tCNN Loss: 0.054430\tRNN Loss: 0.101412"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 8 [500/938 (53%)]\tLin Loss: 0.264348\tCNN Loss: 0.055134\tRNN Loss: 0.155754"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 8 [600/938 (64%)]\tLin Loss: 0.097978\tCNN Loss: 0.059900\tRNN Loss: 0.069061"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 8 [700/938 (75%)]\tLin Loss: 0.286240\tCNN Loss: 0.033550\tRNN Loss: 0.138441"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 8 [800/938 (85%)]\tLin Loss: 0.264325\tCNN Loss: 0.110306\tRNN Loss: 0.219708"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 8 [900/938 (96%)]\tLin Loss: 0.035313\tCNN Loss: 0.058985\tRNN Loss: 0.118607"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2446, Accuracy: 9389/10000 (94%)\n",
        "\n",
        "CNN Test set: Average loss: 0.0649, Accuracy: 9789/10000 (98%)\n",
        "\n",
        "RNN Test set: Average loss: 0.0930, Accuracy: 9728/10000 (97%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 9 [0/938 (0%)]\tLin Loss: 0.063403\tCNN Loss: 0.015717\tRNN Loss: 0.066270\n",
        "Train Epoch: 9 [100/938 (11%)]\tLin Loss: 0.230603\tCNN Loss: 0.036400\tRNN Loss: 0.198702"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 9 [200/938 (21%)]\tLin Loss: 0.098352\tCNN Loss: 0.086021\tRNN Loss: 0.150534"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 9 [300/938 (32%)]\tLin Loss: 0.113760\tCNN Loss: 0.029244\tRNN Loss: 0.105041"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 9 [400/938 (43%)]\tLin Loss: 0.094484\tCNN Loss: 0.095330\tRNN Loss: 0.132806"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 9 [500/938 (53%)]\tLin Loss: 0.037449\tCNN Loss: 0.011439\tRNN Loss: 0.026655"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 9 [600/938 (64%)]\tLin Loss: 0.339929\tCNN Loss: 0.031256\tRNN Loss: 0.062873"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 9 [700/938 (75%)]\tLin Loss: 0.243089\tCNN Loss: 0.123433\tRNN Loss: 0.118179"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 9 [800/938 (85%)]\tLin Loss: 0.312888\tCNN Loss: 0.020145\tRNN Loss: 0.123364"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 9 [900/938 (96%)]\tLin Loss: 0.182434\tCNN Loss: 0.082592\tRNN Loss: 0.088210"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2358, Accuracy: 9372/10000 (94%)\n",
        "\n",
        "CNN Test set: Average loss: 0.0638, Accuracy: 9813/10000 (98%)\n",
        "\n",
        "RNN Test set: Average loss: 0.1026, Accuracy: 9690/10000 (97%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 10 [0/938 (0%)]\tLin Loss: 0.268580\tCNN Loss: 0.050828\tRNN Loss: 0.062420\n",
        "Train Epoch: 10 [100/938 (11%)]\tLin Loss: 0.227971\tCNN Loss: 0.096326\tRNN Loss: 0.169877"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 10 [200/938 (21%)]\tLin Loss: 0.121240\tCNN Loss: 0.163619\tRNN Loss: 0.101825"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 10 [300/938 (32%)]\tLin Loss: 0.080613\tCNN Loss: 0.065729\tRNN Loss: 0.134255"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 10 [400/938 (43%)]\tLin Loss: 0.154748\tCNN Loss: 0.032693\tRNN Loss: 0.053283"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 10 [500/938 (53%)]\tLin Loss: 0.244385\tCNN Loss: 0.075805\tRNN Loss: 0.056768"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 10 [600/938 (64%)]\tLin Loss: 0.070575\tCNN Loss: 0.112529\tRNN Loss: 0.119637"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 10 [700/938 (75%)]\tLin Loss: 0.205998\tCNN Loss: 0.021049\tRNN Loss: 0.023247"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 10 [800/938 (85%)]\tLin Loss: 0.330375\tCNN Loss: 0.047957\tRNN Loss: 0.126589"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 10 [900/938 (96%)]\tLin Loss: 0.195319\tCNN Loss: 0.015600\tRNN Loss: 0.050528"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2335, Accuracy: 9391/10000 (94%)\n",
        "\n",
        "CNN Test set: Average loss: 0.0608, Accuracy: 9822/10000 (98%)\n",
        "\n",
        "RNN Test set: Average loss: 0.0825, Accuracy: 9750/10000 (98%)\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train Epoch: 11 [0/938 (0%)]\tLin Loss: 0.208613\tCNN Loss: 0.021225\tRNN Loss: 0.042695\n",
        "Train Epoch: 11 [100/938 (11%)]\tLin Loss: 0.090961\tCNN Loss: 0.093948\tRNN Loss: 0.126356"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 11 [200/938 (21%)]\tLin Loss: 0.192681\tCNN Loss: 0.038933\tRNN Loss: 0.051002"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 11 [300/938 (32%)]\tLin Loss: 0.051208\tCNN Loss: 0.018922\tRNN Loss: 0.024151"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 11 [400/938 (43%)]\tLin Loss: 0.126089\tCNN Loss: 0.021018\tRNN Loss: 0.058968"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 11 [500/938 (53%)]\tLin Loss: 0.106484\tCNN Loss: 0.010187\tRNN Loss: 0.094108"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 11 [600/938 (64%)]\tLin Loss: 0.085381\tCNN Loss: 0.046725\tRNN Loss: 0.044902"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 11 [700/938 (75%)]\tLin Loss: 0.163296\tCNN Loss: 0.057771\tRNN Loss: 0.126478"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 11 [800/938 (85%)]\tLin Loss: 0.104193\tCNN Loss: 0.009955\tRNN Loss: 0.032286"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Train Epoch: 11 [900/938 (96%)]\tLin Loss: 0.440801\tCNN Loss: 0.100179\tRNN Loss: 0.203562"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Linear Test set: Average loss: 0.2488, Accuracy: 9391/10000 (94%)\n",
        "\n",
        "CNN Test set: Average loss: 0.0596, Accuracy: 9820/10000 (98%)\n",
        "\n",
        "RNN Test set: Average loss: 0.0830, Accuracy: 9756/10000 (98%)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scatters = []\n",
      "for name in ['Linear', 'CNN', 'RNN']:\n",
      "    xs = [i for i in range(1, num_epochs+1)]\n",
      "    ys = [float(i[name])/len(testing.dataset) for i in correct]\n",
      "    scatters.append(go.Scatter(\n",
      "        x = xs,\n",
      "        y = ys,\n",
      "        name=name\n",
      "    ))\n",
      "    print ys\n",
      "iplot(scatters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.1048, 0.9232, 0.9303, 0.9324, 0.9393, 0.9354, 0.936, 0.9346, 0.9389, 0.9372, 0.9391]\n",
        "[0.1028, 0.9598, 0.9669, 0.9635, 0.9706, 0.976, 0.977, 0.9771, 0.9789, 0.9813, 0.982]\n",
        "[0.1157, 0.8907, 0.9449, 0.9527, 0.9607, 0.9657, 0.9712, 0.9699, 0.9728, 0.969, 0.9756]\n"
       ]
      },
      {
       "html": [
        "<div id=\"7d2d0905-a680-41c9-bcfe-fd3619ae64c1\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"7d2d0905-a680-41c9-bcfe-fd3619ae64c1\", [{\"y\": [0.1048, 0.9232, 0.9303, 0.9324, 0.9393, 0.9354, 0.936, 0.9346, 0.9389, 0.9372, 0.9391], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"type\": \"scatter\", \"name\": \"Linear\"}, {\"y\": [0.1028, 0.9598, 0.9669, 0.9635, 0.9706, 0.976, 0.977, 0.9771, 0.9789, 0.9813, 0.982], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"type\": \"scatter\", \"name\": \"CNN\"}, {\"y\": [0.1157, 0.8907, 0.9449, 0.9527, 0.9607, 0.9657, 0.9712, 0.9699, 0.9728, 0.969, 0.9756], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"type\": \"scatter\", \"name\": \"RNN\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x7fd8dd8479d0>"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://pytorch.org/docs/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}